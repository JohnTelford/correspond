The GPT (Generative Pre-trained Transformer) is a type of natural language processing (NLP) model based on the Transformer-Architecture. It was created by OpenAI as a means of facilitating large-scale natural language understanding tasks. The GPT uses a transformer-based neural network to generate text by predicting the next words in a sequence given the previous words. In essence, it is similar to an auto-complete feature seen in many text-editing programs. The GPT has been trained on a large corpus of text and has the ability to generate high-quality text similar to the style of the training data. It can be used for a variety of tasks, such as summarization, question-answering, and context-based text generation.